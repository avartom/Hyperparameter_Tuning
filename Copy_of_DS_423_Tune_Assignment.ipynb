{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "nteract": {
      "version": "0.22.4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGGrt9EYlCqY"
      },
      "source": [
        "\n",
        "\n",
        "# Hyperparameter Tuning Practice\n",
        "\n",
        "## *Data Science Unit 4 Sprint 2 Assignment 3*\n",
        "\n",
        "# Gridsearch Hyperparameters\n",
        "\n",
        "In the guided project, you learned how to use sklearn's `GridsearchCV` and `keras-tuner` libraries to tune the hyperparameters of a neural network model. For your module project you'll continue using these two libraries, however we are going to make things a little more interesting for you. \n",
        "\n",
        "Continue to use TensorFlow Keras & a sample of the [Quickdraw dataset](https://github.com/googlecreativelab/quickdraw-dataset) to build a sketch classification model. The dataset has been sampled to only 10 classes and 10000 observations per class. \n",
        "\n",
        "\n",
        "\n",
        "**Don't forget to switch to GPU on Colab!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7oEgGCV3_hY"
      },
      "source": [
        "## 0.1 Imports and installs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxctNMPb7mNY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "933ea96d-6d44-4fdf-bbf8-d4e5732f9fde"
      },
      "source": [
        "# native python libraries imports \n",
        "import math\n",
        "from time import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# sklearn imports \n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# keras imports \n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.activations import relu, sigmoid\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.utils import get_file\n",
        "\n",
        "# required for compatibility between sklearn and keras\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "# install keras-tuner\n",
        "!pip install keras-tuner\n",
        "from kerastuner.tuners import RandomSearch, BayesianOptimization, Sklearn\n",
        "from kerastuner.engine.hyperparameters import HyperParameters"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.1.3-py3-none-any.whl (135 kB)\n",
            "\u001b[K     |████████████████████████████████| 135 kB 31.4 MB/s \n",
            "\u001b[?25hCollecting kt-legacy\n",
            "  Downloading kt_legacy-1.0.4-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from keras-tuner) (1.21.6)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.8/dist-packages (from keras-tuner) (7.9.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from keras-tuner) (21.3)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.8/dist-packages (from keras-tuner) (2.9.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from keras-tuner) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner) (57.4.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner) (0.7.5)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner) (4.8.0)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 50.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner) (5.7.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner) (4.4.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner) (2.0.10)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner) (2.6.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner) (0.2.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.10->ipython->keras-tuner) (0.8.3)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->keras-tuner) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->keras-tuner) (0.2.5)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->keras-tuner) (3.0.9)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.8/dist-packages (from pexpect->ipython->keras-tuner) (0.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->keras-tuner) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->keras-tuner) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->keras-tuner) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->keras-tuner) (2022.12.7)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras-tuner) (3.19.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras-tuner) (3.4.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras-tuner) (1.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras-tuner) (0.38.4)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras-tuner) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras-tuner) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras-tuner) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras-tuner) (2.15.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras-tuner) (0.6.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras-tuner) (1.51.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (5.2.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard->keras-tuner) (5.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->keras-tuner) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (3.2.2)\n",
            "Installing collected packages: jedi, kt-legacy, keras-tuner\n",
            "Successfully installed jedi-0.18.2 keras-tuner-1.1.3 kt-legacy-1.0.4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-a04cd5b873a9>:25: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
            "  from kerastuner.tuners import RandomSearch, BayesianOptimization, Sklearn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMBS8CRBzYqB"
      },
      "source": [
        "## 0.2 Load quickdraw data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kr8w6IX37mNa"
      },
      "source": [
        "def load_quickdraw10():\n",
        "    \"\"\"\n",
        "    Fill out this doc string, and comment the code, for practice in writing the kind of code that will get you hired. \n",
        "    \"\"\"\n",
        "    \n",
        "    URL_ = \"https://github.com/LambdaSchool/DS-Unit-4-Sprint-2-Neural-Networks/blob/main/quickdraw10.npz?raw=true\"\n",
        "    \n",
        "    path_to_zip = get_file('./quickdraw10.npz', origin=URL_, extract=False)\n",
        "\n",
        "    data = np.load(path_to_zip)\n",
        "    \n",
        "    # normalize your image data\n",
        "    max_pixel_value = 255\n",
        "    X = data['arr_0']/max_pixel_value\n",
        "    Y = data['arr_1']\n",
        "        \n",
        "    return train_test_split(X, Y, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjU5nY3e7mNc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baef4640-8016-40d0-de72-4bef3ac65e59"
      },
      "source": [
        "X_train, X_test, y_train, y_test = load_quickdraw10()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/LambdaSchool/DS-Unit-4-Sprint-2-Neural-Networks/blob/main/quickdraw10.npz?raw=true\n",
            "25421363/25421363 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkvBPoUy7mNd",
        "outputId": "05fc0ce1-9bbd-4d06-8ef0-a7e3a53d851f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(75000, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4dx6VA07mNe",
        "outputId": "268e4584-eec6-4aac-e6bf-6ec508e6c522",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(75000,)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXsWtj8Z7mNf"
      },
      "source": [
        "_____\n",
        "\n",
        "# Experiment 1\n",
        "\n",
        "## Tune Hyperperameters using Enhanced GridsearchCV \n",
        "\n",
        "We are going to use GridsearchCV again to tune a deep learning model however we are going to add some additional functionality to our gridsearch. \n",
        "\n",
        "Specifically, we are going to automate the generation of how many nodes to use in a layer and how many layers to use in a model! \n",
        "\n",
        "By the way, yes, there is a function within a function. Try to not let that bother you. An alternative to this would be to create a class. If you're up for the challenge give it a shot. However, consider this a stretch goal that you come back to after you finish going through this assignment. \n",
        "\n",
        "\n",
        "### Objective \n",
        "\n",
        "The objective of this experiment is to show you how to automate the generation of layers and layer nodes for the purposes of gridsearch. <br>\n",
        "Up until now, we've been manually selecting the number of layers and layer nodes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USXjs7Hk71Hy"
      },
      "source": [
        "# Function to create model, required for KerasClassifier\n",
        "def create_model(n_layers,  first_layer_nodes, last_layer_nodes, act_funct =\"relu\", negative_node_incrementation=True):\n",
        "    \"\"\"\"\n",
        "    Returns a compiled keras model \n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    n_layers: int \n",
        "        number of hidden layers in model \n",
        "        To be clear, this excludes the input and output layer.\n",
        "        \n",
        "    first_layer_nodes: int\n",
        "        Number of nodes in the first hidden layer \n",
        "\n",
        "    last_layer_nodes: int\n",
        "        Number of nodes in the last hidden layer (this is the layer just prior to the output layer)\n",
        "        \n",
        "     act_funct: string \n",
        "         Name of activation function to use in hidden layers (this excludes the output layer)\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    model: keras object \n",
        "    \"\"\"\n",
        "    \n",
        "    def gen_layer_nodes(n_layers, first_layer_nodes, last_layer_nodes, negative_node_incrementation=True):\n",
        "        \"\"\"\n",
        "        Generates and returns the number of nodes in each hidden layer. \n",
        "        To be clear, this excludes the input and output layer. \n",
        "\n",
        "        Note\n",
        "        ----\n",
        "        Number of nodes in each layer is linearly incremented. \n",
        "        For example, gen_layer_nodes(5, 500, 100) will generate [500, 400, 300, 200, 100]\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_layers: int\n",
        "            Number of hidden layers\n",
        "            This values should be 2 or greater \n",
        "\n",
        "        first_layer_nodes: int\n",
        "\n",
        "        last_layer_nodes: int\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        layers: list of ints\n",
        "            Contains number of nodes for each layer \n",
        "        \"\"\"\n",
        "\n",
        "        # throws an error if n_layers is less than 2 \n",
        "        assert n_layers >= 2, \"n_layers must be 2 or greater\"\n",
        "\n",
        "        layers = []\n",
        "\n",
        "        # PROTIP: IF YOU WANT THE NODE INCREMENTATION TO BE SPACED DIFFERENTLY\n",
        "        # THEN YOU'LL NEED TO CHANGE THE WAY THAT IT'S CALCULATED - HAVE FUN!\n",
        "        # when set to True number of nodes are decreased for subsequent layers \n",
        "        # NOTE: the order of the number of nodes doesn't matter\n",
        "        if negative_node_incrementation:\n",
        "            # subtract this amount from previous layer's nodes in order to increment towards smaller numbers \n",
        "            nodes_increment = (last_layer_nodes - first_layer_nodes)/ (n_layers-1)\n",
        "            \n",
        "        # when set to False number of nodes are increased for subsequent layers\n",
        "        else:\n",
        "            # add this amount from previous layer's nodes in order to increment towards larger numbers \n",
        "            nodes_increment = (last_layer_nodes - first_layer_nodes)/ (n_layers-1)\n",
        "\n",
        "        nodes = first_layer_nodes\n",
        "\n",
        "        for i in range(1, n_layers+1):\n",
        "\n",
        "            layers.append(math.ceil(nodes))\n",
        "\n",
        "            # increment nodes for next layer \n",
        "            nodes = nodes + nodes_increment\n",
        "\n",
        "        return layers\n",
        "    \n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    \n",
        "    n_nodes = gen_layer_nodes(n_layers, first_layer_nodes, last_layer_nodes, negative_node_incrementation)\n",
        "    \n",
        "    for i in range(1, n_layers):\n",
        "        if i==1:\n",
        "            model.add(Dense(first_layer_nodes, input_dim=X_train.shape[1], activation=act_funct))\n",
        "        else:\n",
        "            model.add(Dense(n_nodes[i-1], activation=act_funct))\n",
        "            \n",
        "            \n",
        "    # output layer \n",
        "    model.add(Dense(10, # 10 unit/neurons in output layer because we have 10 possible labels to predict  \n",
        "                    activation='softmax')) # use softmax for a label set greater than 2            \n",
        "    \n",
        "    # Compile model\n",
        "    model.compile(loss='sparse_categorical_crossentropy', \n",
        "                  optimizer='adam', # adam is a good default optimizer \n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    # do not include model.fit() inside the create_model function\n",
        "    # KerasClassifier is expecting a complied model \n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YO-x0nqt7mNh"
      },
      "source": [
        "## 1.1 Explore `create_model`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The helper function `gen_layer_nodes()` which is contained inside `create_model()` <br>\n",
        "returns a list containing the number of nodes for each successive layer.<br>\n",
        "\n",
        "Let's check that `gen_layer_nodes()` behaves as expected. <br>\n",
        "In other words, we'll perform a **Unit Test!**"
      ],
      "metadata": {
        "id": "-1hnjQHKW19w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    def gen_layer_nodes(n_layers, first_layer_nodes, last_layer_nodes, negative_node_incrementation=True):\n",
        "        \"\"\"\n",
        "        Generates and returns the number of nodes in each hidden layer. \n",
        "        To be clear, this excludes the input and output layer. \n",
        "\n",
        "        Note\n",
        "        ----\n",
        "        Number of nodes in each layer is linearly incremented. \n",
        "        For example, gen_layer_nodes(5, 500, 100) will generate [500, 400, 300, 200, 100]\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_layers: int\n",
        "            Number of hidden layers\n",
        "            This values should be 2 or greater \n",
        "\n",
        "        first_layer_nodes: int\n",
        "\n",
        "        last_layer_nodes: int\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        layers: list of ints\n",
        "            Contains number of nodes for each layer \n",
        "        \"\"\"\n",
        "\n",
        "        # throws an error if n_layers is less than 2 \n",
        "        assert n_layers >= 2, \"n_layers must be 2 or greater\"\n",
        "\n",
        "        layers = []\n",
        "\n",
        "        # PROTIP: IF YOU WANT THE NODE INCREMENTATION TO BE SPACED DIFFERENTLY\n",
        "        # THEN YOU'LL NEED TO CHANGE THE WAY THAT IT'S CALCULATED - HAVE FUN!\n",
        "        # when set to True number of nodes are decreased for subsequent layers \n",
        "        # NOTE: the order of the number of nodes doesn't matter\n",
        "        if negative_node_incrementation:\n",
        "            # subtract this amount from previous layer's nodes in order to increment towards smaller numbers \n",
        "            nodes_increment = (last_layer_nodes - first_layer_nodes)/ (n_layers-1)\n",
        "            #print(f'nodes increment = {nodes_increment}')\n",
        "            \n",
        "        # when set to False number of nodes are increased for subsequent layers\n",
        "        else:\n",
        "            # add this amount from previous layer's nodes in order to increment towards larger numbers \n",
        "            nodes_increment = (last_layer_nodes - first_layer_nodes)/ (n_layers-1)\n",
        "            #print(f'nodes increment = {nodes_increment}')\n",
        "\n",
        "        nodes = first_layer_nodes\n",
        "\n",
        "        for i in range(1, n_layers+1):\n",
        "\n",
        "            layers.append(math.ceil(nodes))\n",
        "\n",
        "            # increment nodes for next layer \n",
        "            nodes = nodes + nodes_increment\n",
        "\n",
        "        return layers\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YiPXu0p_Qco_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `negative_node_incrementation = True`\n",
        "For this case we want the number of nodes to _decrease_ by a constant number for successive layers. <br>So `first_layer_nodes` must be _larger_ than `last_layer_nodes` "
      ],
      "metadata": {
        "id": "Mj3MrB6jXUMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_layers = 5\n",
        "first_layer_nodes = 500\n",
        "last_layer_nodes = 100\n",
        "negative_node_incrementation = True\n",
        "n_nodes = gen_layer_nodes(n_layers, first_layer_nodes, last_layer_nodes, negative_node_incrementation)\n",
        "print(f'Number of nodes in successive layers: {n_nodes}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4m4jRNllXPPG",
        "outputId": "b8d57289-3dfd-45ba-d6de-1f483fb0d9c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of nodes in successive layers: [500, 400, 300, 200, 100]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `negative_node_incrementation = False`\n",
        "For this case we want the number of nodes to _increase_ by a constant number for successive layers. <br>So `first_layer_nodes` must be _smaller_ than `last_layer_nodes` "
      ],
      "metadata": {
        "id": "ttkaf3g9XhGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_layers = 5\n",
        "first_layer_nodes = 100\n",
        "last_layer_nodes = 500\n",
        "negative_node_incrementation = False\n",
        "n_nodes = gen_layer_nodes(n_layers, first_layer_nodes, last_layer_nodes, negative_node_incrementation)\n",
        "print(f'Number of nodes in successive layers: {n_nodes}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fkrMS8bXQUo",
        "outputId": "35786df2-c872-46be-e651-e2cb16a45f12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of nodes in successive layers: [100, 200, 300, 400, 500]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OK, the Unit Test is passed!"
      ],
      "metadata": {
        "id": "FHuB-bm5Wkpq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's build a few models<br> \n",
        "in order to understand how `create_model()` works in practice. "
      ],
      "metadata": {
        "id": "qO3AjVWOZ6SA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95E85Ug07mNh"
      },
      "source": [
        "### Build a model, setting `negative_node_incrementation = True` \n",
        "\n",
        "Use `create_model` to build a model. \n",
        "\n",
        "- Set `n_layers = 10` \n",
        "- Set `first_layer_nodes = 500`\n",
        "- Set `last_layer_nodes = 100`\n",
        "- Set `act_funct = \"relu\"`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5dcf5c585f07629a03086cf57ba53615",
          "grade": false,
          "grade_id": "cell-86d63e89a21223de",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "x_1REOCY7mNi"
      },
      "source": [
        "# use create_model to create a model \n",
        "model_true = create_model(n_layers=10, first_layer_nodes=500, last_layer_nodes=100, act_funct=\"relu\", negative_node_incrementation=True)\n",
        "\n",
        "# YOUR CODE HERE\n",
        "#raise NotImplementedError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYMwZQ7k7mNi",
        "outputId": "d1287ed1-905e-48df-f613-e6f51e29443f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# run model.summary() and make sure that you understand the model architecture that you just built \n",
        "# Notice in the model summary how the number of nodes have been linearly incremented in decreasing values. \n",
        "model_true.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 500)               392500    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 456)               228456    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 412)               188284    \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 367)               151571    \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 323)               118864    \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 278)               90072     \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 234)               65286     \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 189)               44415     \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 145)               27550     \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 10)                1460      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,308,458\n",
            "Trainable params: 1,308,458\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUc0jfnRm-uh"
      },
      "source": [
        "### Build a model, setting `negative_node_incrementation = False` \n",
        "\n",
        "Use `create_model` to build a model. \n",
        "\n",
        "- Set `n_layers = 10` \n",
        "- Set `first_layer_nodes = 100`\n",
        "- Set `last_layer_nodes = 500`\n",
        "- Set `act_funct = \"relu\"`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5dcf5c585f07629a03086cf57ba53615",
          "grade": false,
          "grade_id": "cell-86d63e89a21223de",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "3_-kqHQtm-ui"
      },
      "source": [
        "# use create_model to create a model \n",
        "model_false = create_model(n_layers=10, first_layer_nodes=100, last_layer_nodes=500, act_funct=\"relu\", negative_node_incrementation=False)\n",
        "# YOUR CODE HERE\n",
        "#raise NotImplementedError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piboKWsNm-uj",
        "outputId": "5d3a173c-0e58-4730-b258-3b861569e5f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# run model.summary() and make sure that you understand the model architecture that you just built \n",
        "# Notice in the model summary how the number of nodes have been linearly incremented in decreasing values. \n",
        "model_false.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_10 (Dense)            (None, 100)               78500     \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 145)               14645     \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 189)               27594     \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 234)               44460     \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 278)               65330     \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 323)               90117     \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 367)               118908    \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 412)               151616    \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 456)               188328    \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 10)                4570      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 784,068\n",
            "Trainable params: 784,068\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBH7AR9p0OXi"
      },
      "source": [
        "## 1.2 Create a grid search using `sklearn`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veloj7Nnlttf"
      },
      "source": [
        "### Hyperparameter search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e2lhZqP7mNn"
      },
      "source": [
        "# define the grid search parameters\n",
        "param_grid = {'n_layers': [2, 3],\n",
        "              'epochs': [3], \n",
        "              \"first_layer_nodes\": [500, 300],\n",
        "              \"last_layer_nodes\": [100, 50]\n",
        "             }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ks_MLPB7mNn",
        "outputId": "1a297709-2996-4f92-9e77-fdbed98fc0a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = KerasClassifier(create_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-22e3fde777af>:1: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  model = KerasClassifier(create_model)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8GKbLJ_7mNn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4deb2533-7b82-49b3-bd64-f95bcfd937d2"
      },
      "source": [
        "%%time\n",
        "# Create Grid Search\n",
        "grid = GridSearchCV(estimator=model, \n",
        "                    param_grid=param_grid, \n",
        "                    n_jobs=-2, \n",
        "                    verbose=1, \n",
        "                    cv=3)\n",
        "\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# Report Results\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 7s 3ms/step - loss: 0.6676 - accuracy: 0.8009\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.4516 - accuracy: 0.8653\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3634 - accuracy: 0.8908\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4705 - accuracy: 0.8632\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6637 - accuracy: 0.8006\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4495 - accuracy: 0.8669\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3623 - accuracy: 0.8911\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4785 - accuracy: 0.8635\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6575 - accuracy: 0.8033\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4495 - accuracy: 0.8659\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3595 - accuracy: 0.8922\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4582 - accuracy: 0.8673\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6368 - accuracy: 0.8049\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4318 - accuracy: 0.8690\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3437 - accuracy: 0.8939\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4441 - accuracy: 0.8670\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6333 - accuracy: 0.8076\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4322 - accuracy: 0.8690\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3466 - accuracy: 0.8929\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4547 - accuracy: 0.8686\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6321 - accuracy: 0.8055\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4317 - accuracy: 0.8693\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3449 - accuracy: 0.8933\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4728 - accuracy: 0.8596\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6669 - accuracy: 0.7999\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.4495 - accuracy: 0.8664\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3590 - accuracy: 0.8928\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4672 - accuracy: 0.8652\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6641 - accuracy: 0.7998\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4498 - accuracy: 0.8659\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.3632 - accuracy: 0.8912\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4863 - accuracy: 0.8615\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6608 - accuracy: 0.8029\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4482 - accuracy: 0.8674\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.3602 - accuracy: 0.8936\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.4540 - accuracy: 0.8670\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6361 - accuracy: 0.8051\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.4333 - accuracy: 0.8680\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3469 - accuracy: 0.8927\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.4525 - accuracy: 0.8678\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6353 - accuracy: 0.8059\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4335 - accuracy: 0.8676\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3459 - accuracy: 0.8942\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.4623 - accuracy: 0.8671\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6321 - accuracy: 0.8065\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4302 - accuracy: 0.8697\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3455 - accuracy: 0.8935\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4783 - accuracy: 0.8632\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6877 - accuracy: 0.7960\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4693 - accuracy: 0.8618\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3836 - accuracy: 0.8862\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4614 - accuracy: 0.8648\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6876 - accuracy: 0.7929\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4700 - accuracy: 0.8612\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3899 - accuracy: 0.8837\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4687 - accuracy: 0.8646\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6829 - accuracy: 0.7967\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4705 - accuracy: 0.8601\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3875 - accuracy: 0.8843\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4728 - accuracy: 0.8634\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6575 - accuracy: 0.7991\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4495 - accuracy: 0.8638\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3674 - accuracy: 0.8870\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4752 - accuracy: 0.8599\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6506 - accuracy: 0.7999\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4445 - accuracy: 0.8650\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3633 - accuracy: 0.8874\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4553 - accuracy: 0.8642\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6530 - accuracy: 0.8023\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4437 - accuracy: 0.8658\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3615 - accuracy: 0.8899\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4775 - accuracy: 0.8588\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6869 - accuracy: 0.7947\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4715 - accuracy: 0.8597\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3882 - accuracy: 0.8840\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4661 - accuracy: 0.8650\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6838 - accuracy: 0.7956\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4707 - accuracy: 0.8602\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3855 - accuracy: 0.8846\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4823 - accuracy: 0.8617\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.6845 - accuracy: 0.7963\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4690 - accuracy: 0.8613\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3823 - accuracy: 0.8872\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4769 - accuracy: 0.8611\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6618 - accuracy: 0.7977\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4524 - accuracy: 0.8629\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3694 - accuracy: 0.8878\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4571 - accuracy: 0.8644\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6516 - accuracy: 0.8012\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4475 - accuracy: 0.8632\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3664 - accuracy: 0.8873\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4696 - accuracy: 0.8615\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6583 - accuracy: 0.8000\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4507 - accuracy: 0.8635\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3663 - accuracy: 0.8875\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4599 - accuracy: 0.8670\n",
            "Epoch 1/3\n",
            "2344/2344 [==============================] - 7s 3ms/step - loss: 0.5858 - accuracy: 0.8208\n",
            "Epoch 2/3\n",
            "2344/2344 [==============================] - 7s 3ms/step - loss: 0.4075 - accuracy: 0.8768\n",
            "Epoch 3/3\n",
            "2344/2344 [==============================] - 7s 3ms/step - loss: 0.3303 - accuracy: 0.8989\n",
            "Best: 0.8660399913787842 using {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 50, 'n_layers': 3}\n",
            "Means: 0.8646533489227295, Stdev: 0.0018631334415004479 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 100, 'n_layers': 2}\n",
            "Means: 0.8650799989700317, Stdev: 0.0039017256778381536 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 100, 'n_layers': 3}\n",
            "Means: 0.8645866513252258, Stdev: 0.0023163870523703637 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 50, 'n_layers': 2}\n",
            "Means: 0.8660399913787842, Stdev: 0.002031999449634334 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 50, 'n_layers': 3}\n",
            "Means: 0.8642799854278564, Stdev: 0.0005959918652815125 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 100, 'n_layers': 2}\n",
            "Means: 0.8610000014305115, Stdev: 0.0023330619984356306 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 100, 'n_layers': 3}\n",
            "Means: 0.8626000086466471, Stdev: 0.0016866601795922191 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 50, 'n_layers': 2}\n",
            "Means: 0.8642933170000712, Stdev: 0.0022703420108813013 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 50, 'n_layers': 3}\n",
            "CPU times: user 7min 14s, sys: 44.7 s, total: 7min 59s\n",
            "Wall time: 8min 16s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfH6okqe7mNo"
      },
      "source": [
        "best_model = grid_result.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Inlda_0w7mNo",
        "outputId": "cb4017b9-7c13-4539-fed7-dd882e89fca1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "best_model.get_params()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'epochs': 3,\n",
              " 'first_layer_nodes': 500,\n",
              " 'last_layer_nodes': 50,\n",
              " 'n_layers': 3,\n",
              " 'build_fn': <function __main__.create_model(n_layers, first_layer_nodes, last_layer_nodes, act_funct='relu', negative_node_incrementation=True)>}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrs3Yib17mNl"
      },
      "source": [
        "Ok, now that we've played around a bit with  `create_model`, let's build a  simpler model that we'll use to run gridsearches. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvegpS1-5yYX"
      },
      "source": [
        "### Build model\n",
        "\n",
        "Use `create_model` to build a model. \n",
        "\n",
        "- Set `n_layers = 2` \n",
        "- Set `first_layer_nodes = 500`\n",
        "- Set `last_layer_nodes = 100`\n",
        "- Set `act_funct = \"relu\"`\n",
        "- Make sure that `negative_node_incrementation = True`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-4ca6c5e51302fd10",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "p-NcKYRr5yYX"
      },
      "source": [
        "# use create_model to create a model \n",
        "model_t = create_model(n_layers=2, first_layer_nodes=500, last_layer_nodes=100, act_funct=\"relu\", negative_node_incrementation=True)\n",
        "###BEGIN SOLUTION\n",
        "# use create_model to create a model \n",
        "\n",
        "###END SOLUTION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICLd6cYN5yYY",
        "outputId": "229b94a1-9102-457e-bd84-49596dd56926"
      },
      "source": [
        "# run model.summary() and make sure that you understand the model architecture that you just built \n",
        "model_t.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_27\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_83 (Dense)            (None, 500)               392500    \n",
            "                                                                 \n",
            " dense_84 (Dense)            (None, 10)                5010      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 397,510\n",
            "Trainable params: 397,510\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwY6GFo85yYY"
      },
      "source": [
        "# define the grid search parameters\n",
        "param_grid = {'n_layers': [2, 3],\n",
        "              'epochs': [3], \n",
        "              \"first_layer_nodes\": [500, 300],\n",
        "              \"last_layer_nodes\": [100, 50]\n",
        "             }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a0iHBqJ5yYY",
        "outputId": "bffa8bfc-f9c6-45ed-eeaa-52c7cba02ee5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = KerasClassifier(create_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-22e3fde777af>:1: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  model = KerasClassifier(create_model)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxpuM3g15yYZ",
        "outputId": "5406da8b-aa45-4a27-f4eb-1562ff6f46f8"
      },
      "source": [
        "%%time\n",
        "# Create Grid Search\n",
        "grid = GridSearchCV(estimator=model, \n",
        "                    param_grid=param_grid, \n",
        "                    n_jobs=-2, \n",
        "                    verbose=1, \n",
        "                    cv=3)\n",
        "\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# Report Results\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6696 - accuracy: 0.7991\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4548 - accuracy: 0.8648\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3636 - accuracy: 0.8921\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4717 - accuracy: 0.8603\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6593 - accuracy: 0.8030\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4487 - accuracy: 0.8664\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3589 - accuracy: 0.8925\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4769 - accuracy: 0.8608\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6631 - accuracy: 0.8017\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.4522 - accuracy: 0.8649\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3624 - accuracy: 0.8921\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4614 - accuracy: 0.8650\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6361 - accuracy: 0.8061\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4281 - accuracy: 0.8695\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3435 - accuracy: 0.8945\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4534 - accuracy: 0.8668\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6332 - accuracy: 0.8060\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4284 - accuracy: 0.8683\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3435 - accuracy: 0.8955\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4533 - accuracy: 0.8704\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6354 - accuracy: 0.8070\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4325 - accuracy: 0.8679\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3436 - accuracy: 0.8950\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4617 - accuracy: 0.8642\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6631 - accuracy: 0.8003\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4503 - accuracy: 0.8682\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3649 - accuracy: 0.8907\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4542 - accuracy: 0.8693\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6645 - accuracy: 0.8005\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4496 - accuracy: 0.8656\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.3609 - accuracy: 0.8925\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4908 - accuracy: 0.8585\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6623 - accuracy: 0.8019\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.4497 - accuracy: 0.8665\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.3594 - accuracy: 0.8919\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4774 - accuracy: 0.8602\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6378 - accuracy: 0.8044\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4331 - accuracy: 0.8689\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3442 - accuracy: 0.8947\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4728 - accuracy: 0.8622\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6316 - accuracy: 0.8056\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4311 - accuracy: 0.8693\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3437 - accuracy: 0.8944\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4716 - accuracy: 0.8637\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6323 - accuracy: 0.8065\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4314 - accuracy: 0.8695\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3479 - accuracy: 0.8922\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4547 - accuracy: 0.8670\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6841 - accuracy: 0.7960\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4711 - accuracy: 0.8597\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.3873 - accuracy: 0.8849\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4734 - accuracy: 0.8624\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6835 - accuracy: 0.7962\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4656 - accuracy: 0.8621\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.3822 - accuracy: 0.8866\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4730 - accuracy: 0.8648\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6880 - accuracy: 0.7950\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.4748 - accuracy: 0.8598\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3896 - accuracy: 0.8851\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4947 - accuracy: 0.8557\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6503 - accuracy: 0.8028\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4468 - accuracy: 0.8644\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3633 - accuracy: 0.8891\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4518 - accuracy: 0.8675\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6541 - accuracy: 0.8014\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4498 - accuracy: 0.8640\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3644 - accuracy: 0.8885\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4644 - accuracy: 0.8624\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6537 - accuracy: 0.8017\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4484 - accuracy: 0.8642\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3653 - accuracy: 0.8889\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4714 - accuracy: 0.8611\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6853 - accuracy: 0.7952\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4729 - accuracy: 0.8593\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3868 - accuracy: 0.8853\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4646 - accuracy: 0.8638\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6815 - accuracy: 0.7963\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4702 - accuracy: 0.8610\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3876 - accuracy: 0.8839\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4801 - accuracy: 0.8606\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.6860 - accuracy: 0.7943\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4732 - accuracy: 0.8608\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3879 - accuracy: 0.8847\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4884 - accuracy: 0.8564\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6636 - accuracy: 0.7984\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4543 - accuracy: 0.8617\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3656 - accuracy: 0.8893\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4709 - accuracy: 0.8614\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6525 - accuracy: 0.8010\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4495 - accuracy: 0.8639\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3665 - accuracy: 0.8892\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4519 - accuracy: 0.8686\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6580 - accuracy: 0.8016\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4494 - accuracy: 0.8635\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3645 - accuracy: 0.8886\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4674 - accuracy: 0.8609\n",
            "Epoch 1/3\n",
            "2344/2344 [==============================] - 7s 3ms/step - loss: 0.5888 - accuracy: 0.8213\n",
            "Epoch 2/3\n",
            "2344/2344 [==============================] - 7s 3ms/step - loss: 0.4085 - accuracy: 0.8755\n",
            "Epoch 3/3\n",
            "2344/2344 [==============================] - 6s 3ms/step - loss: 0.3323 - accuracy: 0.8974\n",
            "Best: 0.8671200076738993 using {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 100, 'n_layers': 3}\n",
            "Means: 0.8620266715685526, Stdev: 0.002083425123602563 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 100, 'n_layers': 2}\n",
            "Means: 0.8671200076738993, Stdev: 0.00257640724163195 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 100, 'n_layers': 3}\n",
            "Means: 0.8626799782117208, Stdev: 0.004719437388388874 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 50, 'n_layers': 2}\n",
            "Means: 0.8642933368682861, Stdev: 0.0020170658416768933 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 50, 'n_layers': 3}\n",
            "Means: 0.8609599868456522, Stdev: 0.0038683629097601853 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 100, 'n_layers': 2}\n",
            "Means: 0.8636799852053324, Stdev: 0.0027405076450958124 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 100, 'n_layers': 3}\n",
            "Means: 0.8602799971898397, Stdev: 0.003045772019766422 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 50, 'n_layers': 2}\n",
            "Means: 0.8636533419291178, Stdev: 0.0035042591096649998 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 50, 'n_layers': 3}\n",
            "CPU times: user 7min 10s, sys: 44.3 s, total: 7min 55s\n",
            "Wall time: 8min 22s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIlpwjag5yYZ"
      },
      "source": [
        "best_model = grid_result.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFvMxmr85yYZ",
        "outputId": "12bead9c-9f45-4e47-e339-62712381fee0"
      },
      "source": [
        "best_model.get_params()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'epochs': 3,\n",
              " 'first_layer_nodes': 500,\n",
              " 'last_layer_nodes': 100,\n",
              " 'n_layers': 3,\n",
              " 'build_fn': <function __main__.create_model(n_layers, first_layer_nodes, last_layer_nodes, act_funct='relu', negative_node_incrementation=True)>}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6azV65Nb7mNo"
      },
      "source": [
        "-----\n",
        "\n",
        "# Experiment 2: Run the Gridsearch Algorithms \n",
        "\n",
        "In this section, we are going to use the same model and dataset in order to benchmark 3 different gridsearch approaches: \n",
        "\n",
        "- Gridsearch\n",
        "- Random Search\n",
        "- Bayesian Optimization. \n",
        "\n",
        "\n",
        "Our goal in this experiment is two-fold. We want to see which appraoch \n",
        "\n",
        "- Scores the highest accuracy\n",
        "- Has the shortest run time \n",
        "\n",
        "We want to see how these 3 gridsearch approaches handle these trade-offs and to give you a sense of those trades offs.\n",
        "\n",
        "### Trade-offs\n",
        "\n",
        "`Gridsearch` will train a model on every single unique hyperparameter combination, this guarantees that you'll get the highest possible accuracy from your parameter set but your gridsearch might have a very long run-time. \n",
        "\n",
        "`Random Search` will randomly sample from your parameter set which, depending on how many samples, the run-time might be significantly cut down but you might or might not sample the parameters that correspond to the heightest possible accuracies. \n",
        "\n",
        "`Bayesian Optimization` has a bit of intelligence built into it's search algorithm but you do need to manually select some parameters which may greatly influence the model learning outcomes. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X41u_hls7mNp"
      },
      "source": [
        "-------\n",
        "### Build our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZ_uyKlj7mNp"
      },
      "source": [
        "# because gridsearching can take a lot of time and we are bench marking 3 different approaches\n",
        "# let's build a simple model to minimize run time \n",
        "\n",
        "def build_model(hp):\n",
        "    \n",
        "    \"\"\"\n",
        "    Returns a complied keras model ready for keras-tuner gridsearch algorithms \n",
        "    \"\"\"\n",
        "    \n",
        "    model = Sequential()\n",
        "    \n",
        "    # hidden layer\n",
        "    model.add(Dense(units=hp.get('units'),activation=hp.get(\"activation\")))\n",
        "    \n",
        "    # output layer\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer=Adam(hp.get('learning_rate')),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYE7rTku7mNp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "2a357daf-401a-480b-87d2-7c760fc18b1e"
      },
      "source": [
        "# build out our hyperparameter dictionary \n",
        "hp = HyperParameters()\n",
        "hp.Int('units', min_value=32, max_value=512, step=32)\n",
        "hp.Choice('learning_rate',values=[1e-1, 1e-2, 1e-3])\n",
        "hp.Choice('activation',values=[\"relu\", \"sigmoid\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'relu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqjp2kHD7mNu"
      },
      "source": [
        "---------\n",
        "## 2.1 Gridsearch Optimization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsNW4rJp7mNu"
      },
      "source": [
        "### Populate a `sklearn` compatible parameter dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJQFKoyL7mNu"
      },
      "source": [
        "# build out our hyperparameter dictionary \n",
        "hyper_parameters = {\n",
        "    # BUG Fix: cast array as list otherwise GridSearchCV will throw error\n",
        "    \"units\": np.arange(32, 512, 32).tolist(),\n",
        "    \"learning_rate\": [1e-1, 1e-2, 1e-3],\n",
        "    \"activation\":[\"relu\", \"sigmoid\"]\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcxV58iC7mNu",
        "outputId": "e4957aef-eb03-4fee-ebcd-3750e753befa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "hyper_parameters"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'units': [32,\n",
              "  64,\n",
              "  96,\n",
              "  128,\n",
              "  160,\n",
              "  192,\n",
              "  224,\n",
              "  256,\n",
              "  288,\n",
              "  320,\n",
              "  352,\n",
              "  384,\n",
              "  416,\n",
              "  448,\n",
              "  480],\n",
              " 'learning_rate': [0.1, 0.01, 0.001],\n",
              " 'activation': ['relu', 'sigmoid']}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOoMg9Ao7mNv"
      },
      "source": [
        "### Build a `sklearn` compatible model function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZFVl-I-7mNv"
      },
      "source": [
        "def build_model(units, learning_rate, activation):\n",
        "    \n",
        "    \"\"\"\n",
        "    Returns a complie keras model ready for keras-tuner gridsearch algorithms \n",
        "    \"\"\"\n",
        "    \n",
        "    model = Sequential()\n",
        "    \n",
        "    # hidden layer\n",
        "    model.add(Dense(units, activation=activation))\n",
        "    \n",
        "    # output layer\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqYmn3QFsqZ_"
      },
      "source": [
        "### Apply the \"wrapper\" to make the model compatible with `sklearn`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABSzrTrH7mNw",
        "outputId": "d81d89c2-14cd-4721-c5f5-ea2f83d1347e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = KerasClassifier(build_fn = build_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-a93dc876c14b>:1: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  model = KerasClassifier(build_fn = build_model)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "tTawllrN7mNw",
        "outputId": "a47372ea-1a82-4393-fb2c-5c73aa98aaa3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# save start time \n",
        "start = time()\n",
        "\n",
        "# Create Grid Search\n",
        "grid = GridSearchCV(estimator=model, \n",
        "                    param_grid=hyper_parameters, \n",
        "                    n_jobs=-2, \n",
        "                    verbose=1, \n",
        "                    cv=3)\n",
        "\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# save end time \n",
        "end = time()\n",
        "\n",
        "# Report Results\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 90 candidates, totalling 270 fits\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.9173 - accuracy: 0.2901\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.1138 - accuracy: 0.2280\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.8480 - accuracy: 0.3128\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.7663 - accuracy: 0.3218\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.9149 - accuracy: 0.3001\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.0053 - accuracy: 0.2430\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.7892 - accuracy: 0.3786\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.7735 - accuracy: 0.3392\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.0648 - accuracy: 0.2487\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.0769 - accuracy: 0.2118\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.9594 - accuracy: 0.2995\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.0137 - accuracy: 0.3068\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.0447 - accuracy: 0.2629\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.1186 - accuracy: 0.1840\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.1117 - accuracy: 0.2386\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9875 - accuracy: 0.2489\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.0314 - accuracy: 0.2689\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9354 - accuracy: 0.3180\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.8972 - accuracy: 0.3344\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.7618 - accuracy: 0.3340\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.9276 - accuracy: 0.3261\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9533 - accuracy: 0.2964\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.9326 - accuracy: 0.3206\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9928 - accuracy: 0.2814\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.8660 - accuracy: 0.3729\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8656 - accuracy: 0.3031\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.9852 - accuracy: 0.3311\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.0737 - accuracy: 0.2886\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 2.2422 - accuracy: 0.2060\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.1499 - accuracy: 0.1682\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.9837 - accuracy: 0.3385\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8738 - accuracy: 0.2866\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.9736 - accuracy: 0.3119\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.4715 - accuracy: 0.2633\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 2.0591 - accuracy: 0.3129\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9045 - accuracy: 0.2808\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.0353 - accuracy: 0.3165\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.1176 - accuracy: 0.2713\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.0016 - accuracy: 0.3082\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.3395 - accuracy: 0.2222\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.0938 - accuracy: 0.2838\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9325 - accuracy: 0.2481\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 2.0615 - accuracy: 0.2916\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.0743 - accuracy: 0.1893\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.0652 - accuracy: 0.2944\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9911 - accuracy: 0.2434\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.9288 - accuracy: 0.3495\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.1306 - accuracy: 0.2803\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 2.0671 - accuracy: 0.3257\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8543 - accuracy: 0.3091\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.0009 - accuracy: 0.3299\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.2556 - accuracy: 0.2530\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.9788 - accuracy: 0.3286\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8140 - accuracy: 0.3370\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 2.1351 - accuracy: 0.3280\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9215 - accuracy: 0.2964\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 2.0511 - accuracy: 0.3452\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9247 - accuracy: 0.3021\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 2.1080 - accuracy: 0.3253\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8311 - accuracy: 0.3128\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 2.0628 - accuracy: 0.3054\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9567 - accuracy: 0.2597\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 2.0820 - accuracy: 0.3318\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.0177 - accuracy: 0.2980\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.9334 - accuracy: 0.3637\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8135 - accuracy: 0.3136\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 2.0072 - accuracy: 0.3601\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9367 - accuracy: 0.3560\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.9962 - accuracy: 0.3537\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8731 - accuracy: 0.3236\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 2.1724 - accuracy: 0.3178\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.1352 - accuracy: 0.2622\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 2.1608 - accuracy: 0.3273\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8048 - accuracy: 0.3390\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.9439 - accuracy: 0.3832\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.4340 - accuracy: 0.2792\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 2.2545 - accuracy: 0.2478\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.0943 - accuracy: 0.2053\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 2.1720 - accuracy: 0.2917\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9610 - accuracy: 0.2577\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 2.1539 - accuracy: 0.3436\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9556 - accuracy: 0.3540\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 2.1449 - accuracy: 0.3128\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9491 - accuracy: 0.2510\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 2.1384 - accuracy: 0.3506\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8594 - accuracy: 0.3296\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 2.1319 - accuracy: 0.3354\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8278 - accuracy: 0.3563\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.1616 - accuracy: 0.3404\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8436 - accuracy: 0.3274\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7877 - accuracy: 0.7588\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6936 - accuracy: 0.7872\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7935 - accuracy: 0.7565\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7141 - accuracy: 0.7855\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7700 - accuracy: 0.7648\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7201 - accuracy: 0.7811\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7643 - accuracy: 0.7653\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6808 - accuracy: 0.7950\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7584 - accuracy: 0.7695\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6904 - accuracy: 0.7938\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7562 - accuracy: 0.7701\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6630 - accuracy: 0.8008\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7552 - accuracy: 0.7704\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6557 - accuracy: 0.8067\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7438 - accuracy: 0.7746\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7114 - accuracy: 0.7880\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7511 - accuracy: 0.7725\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6555 - accuracy: 0.8077\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7381 - accuracy: 0.7766\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6702 - accuracy: 0.7978\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7360 - accuracy: 0.7766\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6793 - accuracy: 0.7998\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7461 - accuracy: 0.7744\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6499 - accuracy: 0.8011\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7399 - accuracy: 0.7768\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6471 - accuracy: 0.8072\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7315 - accuracy: 0.7801\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6543 - accuracy: 0.8044\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7483 - accuracy: 0.7732\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6351 - accuracy: 0.8120\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7448 - accuracy: 0.7744\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6371 - accuracy: 0.8108\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7382 - accuracy: 0.7768\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6763 - accuracy: 0.8044\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7434 - accuracy: 0.7737\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6569 - accuracy: 0.8021\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7410 - accuracy: 0.7770\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6773 - accuracy: 0.7953\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7425 - accuracy: 0.7758\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6536 - accuracy: 0.8049\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7392 - accuracy: 0.7765\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6702 - accuracy: 0.8024\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7389 - accuracy: 0.7780\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6715 - accuracy: 0.7950\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7345 - accuracy: 0.7790\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6553 - accuracy: 0.8003\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7371 - accuracy: 0.7778\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6537 - accuracy: 0.8053\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7437 - accuracy: 0.7794\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6782 - accuracy: 0.8047\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7379 - accuracy: 0.7778\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6505 - accuracy: 0.8053\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7413 - accuracy: 0.7786\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6925 - accuracy: 0.7984\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7474 - accuracy: 0.7756\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6629 - accuracy: 0.8053\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7440 - accuracy: 0.7767\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6732 - accuracy: 0.8051\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7355 - accuracy: 0.7789\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6825 - accuracy: 0.7976\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7458 - accuracy: 0.7749\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7004 - accuracy: 0.7920\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7427 - accuracy: 0.7763\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6708 - accuracy: 0.8012\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7358 - accuracy: 0.7755\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6695 - accuracy: 0.8075\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7472 - accuracy: 0.7740\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6719 - accuracy: 0.7998\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7419 - accuracy: 0.7753\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6606 - accuracy: 0.8042\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7342 - accuracy: 0.7787\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6520 - accuracy: 0.8085\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7492 - accuracy: 0.7758\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6673 - accuracy: 0.8026\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7477 - accuracy: 0.7742\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6513 - accuracy: 0.8076\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7441 - accuracy: 0.7770\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6768 - accuracy: 0.8047\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7509 - accuracy: 0.7754\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6538 - accuracy: 0.8052\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7432 - accuracy: 0.7753\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6547 - accuracy: 0.8032\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7425 - accuracy: 0.7788\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6660 - accuracy: 0.8074\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7522 - accuracy: 0.7736\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6599 - accuracy: 0.8090\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7346 - accuracy: 0.7813\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6961 - accuracy: 0.8015\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7393 - accuracy: 0.7817\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6766 - accuracy: 0.7995\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8802 - accuracy: 0.7393\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7384 - accuracy: 0.7878\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8612 - accuracy: 0.7440\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7145 - accuracy: 0.7886\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8600 - accuracy: 0.7457\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7170 - accuracy: 0.7871\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8020 - accuracy: 0.7606\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6698 - accuracy: 0.8035\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7821 - accuracy: 0.7663\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6471 - accuracy: 0.8090\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8099 - accuracy: 0.7621\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6593 - accuracy: 0.8102\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7718 - accuracy: 0.7708\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6079 - accuracy: 0.8241\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7570 - accuracy: 0.7753\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6283 - accuracy: 0.8116\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7731 - accuracy: 0.7697\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6296 - accuracy: 0.8116\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7395 - accuracy: 0.7804\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5741 - accuracy: 0.8320\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7405 - accuracy: 0.7803\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6032 - accuracy: 0.8240\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7356 - accuracy: 0.7802\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5936 - accuracy: 0.8241\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7321 - accuracy: 0.7825\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5822 - accuracy: 0.8285\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7215 - accuracy: 0.7851\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6197 - accuracy: 0.8166\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7333 - accuracy: 0.7814\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5834 - accuracy: 0.8302\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7167 - accuracy: 0.7862\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5601 - accuracy: 0.8354\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7125 - accuracy: 0.7852\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5815 - accuracy: 0.8243\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7097 - accuracy: 0.7886\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5583 - accuracy: 0.8365\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7102 - accuracy: 0.7886\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5613 - accuracy: 0.8364\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7038 - accuracy: 0.7893\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5686 - accuracy: 0.8342\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6991 - accuracy: 0.7937\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5485 - accuracy: 0.8381\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7038 - accuracy: 0.7890\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5680 - accuracy: 0.8337\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6949 - accuracy: 0.7932\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5465 - accuracy: 0.8385\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6918 - accuracy: 0.7932\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5517 - accuracy: 0.8366\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6940 - accuracy: 0.7927\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5469 - accuracy: 0.8380\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6935 - accuracy: 0.7929\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5534 - accuracy: 0.8361\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6907 - accuracy: 0.7956\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5605 - accuracy: 0.8338\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6879 - accuracy: 0.7955\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5385 - accuracy: 0.8398\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6840 - accuracy: 0.7956\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5436 - accuracy: 0.8415\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6754 - accuracy: 0.7991\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5417 - accuracy: 0.8368\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6792 - accuracy: 0.7978\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5317 - accuracy: 0.8437\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6795 - accuracy: 0.7957\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5453 - accuracy: 0.8369\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6815 - accuracy: 0.7952\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5478 - accuracy: 0.8372\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6753 - accuracy: 0.7990\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5258 - accuracy: 0.8446\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6714 - accuracy: 0.8008\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5602 - accuracy: 0.8335\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6750 - accuracy: 0.7991\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5355 - accuracy: 0.8420\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6727 - accuracy: 0.7990\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5286 - accuracy: 0.8446\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6635 - accuracy: 0.8018\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5257 - accuracy: 0.8464\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6699 - accuracy: 0.8006\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5448 - accuracy: 0.8386\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6718 - accuracy: 0.7995\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5209 - accuracy: 0.8467\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6658 - accuracy: 0.8020\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5354 - accuracy: 0.8438\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6620 - accuracy: 0.8034\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5318 - accuracy: 0.8402\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6666 - accuracy: 0.8005\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5386 - accuracy: 0.8376\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6632 - accuracy: 0.7989\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5437 - accuracy: 0.8399\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6666 - accuracy: 0.8017\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5341 - accuracy: 0.8424\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.0835 - accuracy: 0.6507\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.0149 - accuracy: 0.6795\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.1847 - accuracy: 0.6159\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.1268 - accuracy: 0.6382\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.1820 - accuracy: 0.6204\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.1546 - accuracy: 0.6484\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.0943 - accuracy: 0.6575\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.9850 - accuracy: 0.6873\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.1486 - accuracy: 0.6389\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.0265 - accuracy: 0.6874\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.1609 - accuracy: 0.6390\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.1579 - accuracy: 0.6254\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.2247 - accuracy: 0.6232\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.4061 - accuracy: 0.5900\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.1121 - accuracy: 0.6551\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.1462 - accuracy: 0.6561\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.1822 - accuracy: 0.6284\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.2316 - accuracy: 0.6239\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.2729 - accuracy: 0.6173\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.1010 - accuracy: 0.6575\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.1815 - accuracy: 0.6318\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.1651 - accuracy: 0.6427\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.2859 - accuracy: 0.6082\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.1638 - accuracy: 0.6490\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.2732 - accuracy: 0.6217\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.3726 - accuracy: 0.6061\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.2823 - accuracy: 0.6172\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.0835 - accuracy: 0.6622\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.3110 - accuracy: 0.6158\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.3157 - accuracy: 0.6483\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.2972 - accuracy: 0.6194\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.1896 - accuracy: 0.6553\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.3018 - accuracy: 0.6101\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.3499 - accuracy: 0.6273\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.2617 - accuracy: 0.6289\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.1806 - accuracy: 0.6430\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.2092 - accuracy: 0.6303\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.1973 - accuracy: 0.6553\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.4158 - accuracy: 0.6096\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.1773 - accuracy: 0.6643\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.1990 - accuracy: 0.6346\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.0869 - accuracy: 0.6619\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.5435 - accuracy: 0.5750\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.7866 - accuracy: 0.5064\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.3103 - accuracy: 0.6169\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.3346 - accuracy: 0.6239\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.2399 - accuracy: 0.6245\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.1565 - accuracy: 0.6663\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.5676 - accuracy: 0.5868\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.2256 - accuracy: 0.6526\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.2208 - accuracy: 0.6365\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.2243 - accuracy: 0.6592\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.4400 - accuracy: 0.5820\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.1072 - accuracy: 0.6632\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.4458 - accuracy: 0.5898\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.5341 - accuracy: 0.5961\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.4009 - accuracy: 0.6112\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.1091 - accuracy: 0.6859\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.4326 - accuracy: 0.6084\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.3521 - accuracy: 0.6359\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.5872 - accuracy: 0.5895\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.1945 - accuracy: 0.6553\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.3575 - accuracy: 0.6178\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.2685 - accuracy: 0.6442\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.5835 - accuracy: 0.5884\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.6490 - accuracy: 0.5733\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.5926 - accuracy: 0.5662\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.4900 - accuracy: 0.6125\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.3669 - accuracy: 0.6162\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.2402 - accuracy: 0.6369\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.4072 - accuracy: 0.6072\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.0251 - accuracy: 0.6701\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.7278 - accuracy: 0.5813\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.2180 - accuracy: 0.6726\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.3546 - accuracy: 0.6195\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.2392 - accuracy: 0.6544\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.6788 - accuracy: 0.5861\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.5601 - accuracy: 0.6264\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.5955 - accuracy: 0.5839\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.7763 - accuracy: 0.6299\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.3943 - accuracy: 0.6118\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.2255 - accuracy: 0.6299\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.3661 - accuracy: 0.5997\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.1373 - accuracy: 0.6742\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.7456 - accuracy: 0.5874\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.0405 - accuracy: 0.5790\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.4378 - accuracy: 0.6057\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.2046 - accuracy: 0.6561\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.9484 - accuracy: 0.5528\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8858 - accuracy: 0.6302\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7948 - accuracy: 0.7585\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7039 - accuracy: 0.7843\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7951 - accuracy: 0.7569\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7106 - accuracy: 0.7831\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7978 - accuracy: 0.7563\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6969 - accuracy: 0.7902\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7360 - accuracy: 0.7737\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6311 - accuracy: 0.8088\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7340 - accuracy: 0.7757\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6380 - accuracy: 0.8078\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7293 - accuracy: 0.7761\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6323 - accuracy: 0.8094\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7071 - accuracy: 0.7814\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6079 - accuracy: 0.8148\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7053 - accuracy: 0.7818\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6079 - accuracy: 0.8158\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7054 - accuracy: 0.7845\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6166 - accuracy: 0.8124\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6977 - accuracy: 0.7836\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5928 - accuracy: 0.8185\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7049 - accuracy: 0.7836\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6088 - accuracy: 0.8159\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6933 - accuracy: 0.7881\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6181 - accuracy: 0.8123\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6957 - accuracy: 0.7863\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5917 - accuracy: 0.8186\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6980 - accuracy: 0.7858\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6114 - accuracy: 0.8152\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7000 - accuracy: 0.7828\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5993 - accuracy: 0.8185\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6906 - accuracy: 0.7865\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5913 - accuracy: 0.8203\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6838 - accuracy: 0.7900\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6053 - accuracy: 0.8173\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6878 - accuracy: 0.7885\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5968 - accuracy: 0.8196\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6903 - accuracy: 0.7864\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5838 - accuracy: 0.8206\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6877 - accuracy: 0.7879\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5994 - accuracy: 0.8182\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6884 - accuracy: 0.7897\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5990 - accuracy: 0.8178\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6880 - accuracy: 0.7881\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6109 - accuracy: 0.8164\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6830 - accuracy: 0.7900\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5936 - accuracy: 0.8220\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6898 - accuracy: 0.7876\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6040 - accuracy: 0.8159\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6922 - accuracy: 0.7868\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6265 - accuracy: 0.8115\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6848 - accuracy: 0.7881\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6255 - accuracy: 0.8085\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6814 - accuracy: 0.7911\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5701 - accuracy: 0.8251\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6837 - accuracy: 0.7892\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5992 - accuracy: 0.8191\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6854 - accuracy: 0.7881\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6178 - accuracy: 0.8086\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6852 - accuracy: 0.7914\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5979 - accuracy: 0.8170\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6895 - accuracy: 0.7883\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6221 - accuracy: 0.8128\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6818 - accuracy: 0.7899\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6222 - accuracy: 0.8110\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6861 - accuracy: 0.7906\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6138 - accuracy: 0.8169\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6883 - accuracy: 0.7894\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5705 - accuracy: 0.8246\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6850 - accuracy: 0.7886\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5679 - accuracy: 0.8312\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6855 - accuracy: 0.7910\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6040 - accuracy: 0.8230\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6940 - accuracy: 0.7875\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5865 - accuracy: 0.8196\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6921 - accuracy: 0.7876\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6454 - accuracy: 0.8042\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6898 - accuracy: 0.7886\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6065 - accuracy: 0.8145\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6939 - accuracy: 0.7861\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6400 - accuracy: 0.8014\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6853 - accuracy: 0.7913\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5858 - accuracy: 0.8251\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6878 - accuracy: 0.7885\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6092 - accuracy: 0.8111\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6928 - accuracy: 0.7872\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5635 - accuracy: 0.8261\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6964 - accuracy: 0.7859\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6144 - accuracy: 0.8137\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6913 - accuracy: 0.7902\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5862 - accuracy: 0.8188\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.0387 - accuracy: 0.7070\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.8139 - accuracy: 0.7601\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.0371 - accuracy: 0.7059\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.8123 - accuracy: 0.7631\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.0129 - accuracy: 0.7187\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7977 - accuracy: 0.7636\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.9515 - accuracy: 0.7220\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7531 - accuracy: 0.7765\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.9410 - accuracy: 0.7295\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7642 - accuracy: 0.7711\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.9444 - accuracy: 0.7249\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7579 - accuracy: 0.7770\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.9003 - accuracy: 0.7365\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7251 - accuracy: 0.7846\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.9009 - accuracy: 0.7357\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.7271 - accuracy: 0.7852\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8892 - accuracy: 0.7395\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7362 - accuracy: 0.7822\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8712 - accuracy: 0.7446\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6974 - accuracy: 0.7929\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8698 - accuracy: 0.7437\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7152 - accuracy: 0.7874\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8669 - accuracy: 0.7445\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7200 - accuracy: 0.7819\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8595 - accuracy: 0.7465\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6933 - accuracy: 0.7932\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8540 - accuracy: 0.7480\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7021 - accuracy: 0.7940\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8513 - accuracy: 0.7502\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6948 - accuracy: 0.7950\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8480 - accuracy: 0.7481\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6846 - accuracy: 0.7983\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8419 - accuracy: 0.7521\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6896 - accuracy: 0.7990\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8398 - accuracy: 0.7538\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6944 - accuracy: 0.7925\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8425 - accuracy: 0.7505\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6750 - accuracy: 0.7982\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8370 - accuracy: 0.7523\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6874 - accuracy: 0.7984\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8372 - accuracy: 0.7542\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6844 - accuracy: 0.7993\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8381 - accuracy: 0.7517\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6823 - accuracy: 0.7970\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8250 - accuracy: 0.7541\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6734 - accuracy: 0.8020\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8224 - accuracy: 0.7561\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.7049 - accuracy: 0.7864\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8336 - accuracy: 0.7507\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6840 - accuracy: 0.7935\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8230 - accuracy: 0.7554\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6783 - accuracy: 0.8002\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8271 - accuracy: 0.7549\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7011 - accuracy: 0.7933\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8273 - accuracy: 0.7530\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6725 - accuracy: 0.7999\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8170 - accuracy: 0.7576\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6854 - accuracy: 0.7922\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8264 - accuracy: 0.7564\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6839 - accuracy: 0.8004\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8278 - accuracy: 0.7517\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6759 - accuracy: 0.8001\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8145 - accuracy: 0.7580\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6727 - accuracy: 0.8002\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8196 - accuracy: 0.7541\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7120 - accuracy: 0.7795\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8294 - accuracy: 0.7530\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6629 - accuracy: 0.8047\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8193 - accuracy: 0.7568\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6713 - accuracy: 0.7999\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8244 - accuracy: 0.7549\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6844 - accuracy: 0.7994\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8237 - accuracy: 0.7559\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6709 - accuracy: 0.8023\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8155 - accuracy: 0.7553\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6699 - accuracy: 0.8016\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8099 - accuracy: 0.7584\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6719 - accuracy: 0.8030\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8194 - accuracy: 0.7532\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6597 - accuracy: 0.8020\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8151 - accuracy: 0.7583\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6742 - accuracy: 0.8020\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8107 - accuracy: 0.7596\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6710 - accuracy: 0.8036\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8199 - accuracy: 0.7549\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6535 - accuracy: 0.8078\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8132 - accuracy: 0.7575\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6644 - accuracy: 0.8037\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8092 - accuracy: 0.7588\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6813 - accuracy: 0.7969\n",
            "2344/2344 [==============================] - 7s 3ms/step - loss: 0.6155 - accuracy: 0.8170\n",
            "Best: 0.8435733318328857 using {'activation': 'relu', 'learning_rate': 0.001, 'units': 448}\n",
            "Means: 0.2642666647831599, Stdev: 0.04111012070132573 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 32}\n",
            "Means: 0.28592000404993695, Stdev: 0.05402668814295268 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 64}\n",
            "Means: 0.25030666093031567, Stdev: 0.054714052795063424 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 96}\n",
            "Means: 0.3039466639359792, Stdev: 0.022120161939072666 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 128}\n",
            "Means: 0.25329333047072095, Stdev: 0.06046296006618941 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 160}\n",
            "Means: 0.2768933375676473, Stdev: 0.009905274784287783 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 192}\n",
            "Means: 0.24718666076660156, Stdev: 0.02006401070438821 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 224}\n",
            "Means: 0.23765332996845245, Stdev: 0.03737216143378332 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 256}\n",
            "Means: 0.2997066577275594, Stdev: 0.03490081120774871 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 288}\n",
            "Means: 0.30373332897822064, Stdev: 0.0067965768908981004 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 320}\n",
            "Means: 0.29044000307718915, Stdev: 0.022645890502469342 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 352}\n",
            "Means: 0.3139466643333435, Stdev: 0.038870133262843054 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 384}\n",
            "Means: 0.2744933267434438, Stdev: 0.05465971283144666 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 416}\n",
            "Means: 0.2875333329041799, Stdev: 0.047050801892263294 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 448}\n",
            "Means: 0.3377733329931895, Stdev: 0.013147460208980102 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 480}\n",
            "Means: 0.7846266825993856, Stdev: 0.002577106240499488 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 32}\n",
            "Means: 0.7965466777483622, Stdev: 0.0030751264595045534 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 64}\n",
            "Means: 0.8007733424504598, Stdev: 0.009069598993943422 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 96}\n",
            "Means: 0.7995466589927673, Stdev: 0.001347517329510481 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 128}\n",
            "Means: 0.8078533212343851, Stdev: 0.003153043298269838 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 160}\n",
            "Means: 0.805733323097229, Stdev: 0.0036742678971090916 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 192}\n",
            "Means: 0.8008666634559631, Stdev: 0.0040506753922629375 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 224}\n",
            "Means: 0.800213356812795, Stdev: 0.004213800700199659 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 256}\n",
            "Means: 0.8027999997138977, Stdev: 0.0031491031404673616 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 288}\n",
            "Means: 0.8026666839917501, Stdev: 0.00355532222086429 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 320}\n",
            "Means: 0.8002133170763651, Stdev: 0.006355028715116945 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 352}\n",
            "Means: 0.8041866620381674, Stdev: 0.0035274875484671135 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 384}\n",
            "Means: 0.8049733440081278, Stdev: 0.0020679969667527884 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 416}\n",
            "Means: 0.8052399953206381, Stdev: 0.0017318874284646777 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 448}\n",
            "Means: 0.8033200105031332, Stdev: 0.00409852125928077 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 480}\n",
            "Means: 0.7878400087356567, Stdev: 0.0006048864659033407 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 32}\n",
            "Means: 0.8075733383496603, Stdev: 0.002907690425508847 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 64}\n",
            "Means: 0.8157733480135599, Stdev: 0.005873705845208199 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 96}\n",
            "Means: 0.8267066677411398, Stdev: 0.003771809301412266 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 128}\n",
            "Means: 0.825106680393219, Stdev: 0.006054089922524203 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 160}\n",
            "Means: 0.8320666750272115, Stdev: 0.005524947838287152 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 192}\n",
            "Means: 0.8362133105595907, Stdev: 0.0016057747023210903 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 224}\n",
            "Means: 0.8362799882888794, Stdev: 0.0019922593728393606 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 256}\n",
            "Means: 0.8359600106875101, Stdev: 0.0017493501509902511 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 288}\n",
            "Means: 0.8393733302752177, Stdev: 0.0019389034110313952 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 320}\n",
            "Means: 0.8392666776974996, Stdev: 0.0031516957228911557 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 352}\n",
            "Means: 0.8400533397992452, Stdev: 0.004736515496128149 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 384}\n",
            "Means: 0.8431866765022278, Stdev: 0.003346073742757386 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 416}\n",
            "Means: 0.8435733318328857, Stdev: 0.0026847259527198277 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 448}\n",
            "Means: 0.8399866620699564, Stdev: 0.0019438447361631657 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 480}\n",
            "Means: 0.6553333401679993, Stdev: 0.017574727960131313 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 32}\n",
            "Means: 0.6667066613833109, Stdev: 0.029208233839307384 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 64}\n",
            "Means: 0.6233333349227905, Stdev: 0.027012902107829136 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 96}\n",
            "Means: 0.649733324845632, Stdev: 0.006061937590856175 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 128}\n",
            "Means: 0.638866662979126, Stdev: 0.0238661189863493 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 160}\n",
            "Means: 0.6418799956639608, Stdev: 0.011476628302286778 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 192}\n",
            "Means: 0.6605066657066345, Stdev: 0.0038234844481086754 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 224}\n",
            "Means: 0.5988933444023132, Stdev: 0.0676272034088632 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 256}\n",
            "Means: 0.6583466728528341, Stdev: 0.004385481759114482 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 288}\n",
            "Means: 0.6392800013224283, Stdev: 0.03673944337058788 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 320}\n",
            "Means: 0.6242799957593282, Stdev: 0.036314949142312085 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 352}\n",
            "Means: 0.6398133238156637, Stdev: 0.023606411448418456 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 384}\n",
            "Means: 0.6511600017547607, Stdev: 0.019019356011100278 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 416}\n",
            "Means: 0.6446400086085001, Stdev: 0.020873788683000414 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 448}\n",
            "Means: 0.621773342291514, Stdev: 0.03206430979868913 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 480}\n",
            "Means: 0.7858799894650778, Stdev: 0.0031191469730138172 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 32}\n",
            "Means: 0.80867999792099, Stdev: 0.0006556525501321716 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 64}\n",
            "Means: 0.8143200079600016, Stdev: 0.0014543828009636421 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 96}\n",
            "Means: 0.815560003121694, Stdev: 0.002557502792624442 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 128}\n",
            "Means: 0.8174399932225546, Stdev: 0.0016129592473932263 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 160}\n",
            "Means: 0.8190666635831197, Stdev: 0.001290103597576709 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 192}\n",
            "Means: 0.8189066648483276, Stdev: 0.0012365033968939716 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 224}\n",
            "Means: 0.8181066711743673, Stdev: 0.0027599833384291048 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 256}\n",
            "Means: 0.8150399923324585, Stdev: 0.007204207977477879 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 288}\n",
            "Means: 0.8148933251698812, Stdev: 0.004536576527622909 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 320}\n",
            "Means: 0.8135999838511149, Stdev: 0.0024599178225018373 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 352}\n",
            "Means: 0.826253334681193, Stdev: 0.0035245966470304766 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 384}\n",
            "Means: 0.8127999901771545, Stdev: 0.0064035808197472075 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 416}\n",
            "Means: 0.8125199874242147, Stdev: 0.009753261216077558 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 448}\n",
            "Means: 0.8195466796557108, Stdev: 0.005103141460656691 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 480}\n",
            "Means: 0.7622666557629904, Stdev: 0.0015327051485674857 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 32}\n",
            "Means: 0.7748533487319946, Stdev: 0.002674188236773216 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 64}\n",
            "Means: 0.7839999993642172, Stdev: 0.0012716551382344285 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 96}\n",
            "Means: 0.7874000072479248, Stdev: 0.004490811093832959 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 128}\n",
            "Means: 0.7940533558527628, Stdev: 0.000719508168421489 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 160}\n",
            "Means: 0.7965866724650065, Stdev: 0.002918701486105358 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 192}\n",
            "Means: 0.7986133297284445, Stdev: 0.0004814706266948164 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 224}\n",
            "Means: 0.7951066692670187, Stdev: 0.006518088103875164 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 256}\n",
            "Means: 0.7956799864768982, Stdev: 0.003225890409847222 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 288}\n",
            "Means: 0.7975333531697592, Stdev: 0.003748964389180977 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 320}\n",
            "Means: 0.7932533224423727, Stdev: 0.009739346688932312 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 352}\n",
            "Means: 0.8013333280881246, Stdev: 0.0024027338309435455 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 384}\n",
            "Means: 0.8022799889246622, Stdev: 0.0005878728897650758 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 416}\n",
            "Means: 0.8025333285331726, Stdev: 0.0007827108579686608 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 448}\n",
            "Means: 0.8028266628583273, Stdev: 0.004502597049769171 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 480}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THKMZLNv7mNw"
      },
      "source": [
        "# total run time \n",
        "total_run_time_in_miniutes = (end - start)/60\n",
        "total_run_time_in_miniutes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XgJsrZb7mNx"
      },
      "source": [
        "grid_result.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYufbSI87mNx"
      },
      "source": [
        "# because all other optimization approaches are reporting test set score\n",
        "# let's calculate the test set score in this case \n",
        "best_model = grid_result.best_estimator_\n",
        "test_acc = best_model.score(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlR-pVwP7mNx"
      },
      "source": [
        "test_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gj4jJ0Qm7mNx"
      },
      "source": [
        " ### Results\n",
        " \n",
        "Identify and write the the best performing hyperparameter combination and model score. \n",
        " \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "9577db883482c6cded3836e5cfbf5a74",
          "grade": true,
          "grade_id": "cell-eb06d682d2790f6e",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "10px3N2q7mNx"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zu-lBWph7mNq"
      },
      "source": [
        "------\n",
        "## 2.2 Random Search with `keras-tuner`\n",
        "\n",
        "Be sure to check out the [**docs for Keras-Tuner**](https://keras-team.github.io/keras-tuner/documentation/tuners/). Here you can read about the input parameters for the `RandomSearch` tuner."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "aaff9aae33845f374e15f2381719d83a",
          "grade": false,
          "grade_id": "cell-8c1dfb9b6d12bea2",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "8DApqLli7mNq"
      },
      "source": [
        "# how many unique hyperparameter combinations do we have? \n",
        "# HINT: take the product of the number of possible values for each hyperparameter \n",
        "# save your answer to n_unique_hparam_combos\n",
        "\n",
        "# YOUR CODE HERE\n",
        "n_unique_hparam_combos = \n",
        "raise NotImplementedError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a9d628451e83431e1b52da10eccf2c00",
          "grade": false,
          "grade_id": "cell-1fa83950bb2d5f92",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "m1UKRA597mNq"
      },
      "source": [
        "# how many of these do we want to randomly sample?\n",
        "# let's pick 25% of n_unique_hparam_combos param combos to sample\n",
        "# save this number to n_param_combos_to_sample\n",
        "\n",
        "# YOUR CODE HERE\n",
        "fraction_to_sample = 0.25\n",
        "n_param_combos_to_sample = \n",
        "# raise NotImplementedError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TzaNnzoQU4U"
      },
      "source": [
        "### Instantiate a `RandomSearch()` object for your grid search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9PCHLBWQPcb"
      },
      "source": [
        "random_tuner = RandomSearch(\n",
        "            build_model,\n",
        "            objective='val_accuracy',\n",
        "            max_trials=n_param_combos_to_sample, # number of times to sample the parameter set and build a model \n",
        "            seed=1234,\n",
        "            hyperparameters=hp, # pass in our hyperparameter dictionary\n",
        "            directory='./keras-tuner-trial',\n",
        "            project_name='random_search')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGFdv1qE7mNr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4fb1482-c4bc-40cd-c426-d8248e67341f"
      },
      "source": [
        " # take note of Total elapsed time in print out -- took ~10 minutes without GPU\n",
        "random_tuner.search(X_train, y_train,\n",
        "                    epochs=3,\n",
        "                    validation_data=(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 20 Complete [00h 00m 34s]\n",
            "val_accuracy: 0.8131999969482422\n",
            "\n",
            "Best val_accuracy So Far: 0.8730000257492065\n",
            "Total elapsed time: 00h 10m 09s\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNBUhIe97mNr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a12fb52d-eec7-4b72-a2d0-cfb8482ade48"
      },
      "source": [
        "# identify the best score and hyperparamter (should be at the top since scores are ranked)\n",
        "random_tuner.results_summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results summary\n",
            "Results in ./keras-tuner-trial/random_search\n",
            "Showing 10 best trials\n",
            "Objective(name='val_accuracy', direction='max')\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 384\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8730000257492065\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 352\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8729199767112732\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 448\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8636400103569031\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 480\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8621199727058411\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 416\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8591200113296509\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 288\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8572400212287903\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 224\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8525599837303162\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 320\n",
            "learning_rate: 0.01\n",
            "activation: sigmoid\n",
            "Score: 0.8378400206565857\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 160\n",
            "learning_rate: 0.01\n",
            "activation: sigmoid\n",
            "Score: 0.8370400071144104\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 192\n",
            "learning_rate: 0.01\n",
            "activation: sigmoid\n",
            "Score: 0.8350800275802612\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "FRpQVXBE7mNr"
      },
      "source": [
        " ### Results\n",
        " \n",
        "Identify and write the the best performing hyperparameter combination and model score. \n",
        "Note that because this is Random Search, multiple runs might have slighly different outcomes. \n",
        " \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "f084b5d373f8589a1de8d6d4473b974a",
          "grade": true,
          "grade_id": "cell-5527738b6382c164",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "aQjMc84c7mNs"
      },
      "source": [
        "best model score is 0.8730000257492065. Hyperparameters: units: 352\n",
        "learning_rate: 0.001\n",
        "activation: relu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXjW7eYA7mNs"
      },
      "source": [
        "------\n",
        "## 2.3 Bayesian Optimization with `keras-tuner`\n",
        "\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/0/02/GpParBayesAnimationSmall.gif)\n",
        "\n",
        "Be sure to check out the [**docs for Keras-Tuner**](https://keras-team.github.io/keras-tuner/documentation/tuners/). Here you can read about the input parameters for the `BayesianOptimization` tuner.\n",
        "\n",
        "Pay special attention to these `BayesianOptimization` parameters: `num_initial_points` and `beta`. \n",
        "\n",
        "`num_initial_points`: \n",
        "\n",
        "Number of randomly selected hyperparameter combinations to try before applying bayesian probability to determine liklihood of which param combo to try next based on expected improvement\n",
        "\n",
        "\n",
        "`beta`: \n",
        "\n",
        "Larger values means more willing to explore new hyperparameter combinations (analogous to searching for the global minimum in Gradient Descent), smaller values means that it is less willing to try new hyperparameter combinations (analogous to getting stuck in a local minimum in Gradient Descent). \n",
        "\n",
        "As a start, error on the side of larger values. What defines a small or large value you ask? That question would pull us into the mathematical intricacies of Bayesian Optimization and Gaussian Processes. For simplicity, notice that the default value is 2.6 and work from there. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NXjQBn47mNs"
      },
      "source": [
        "# we know that 24 samples is about 25% of 96 possible hyper-parameter combos\n",
        "# let's set up a run with the same parameters we used for RandomSearch() so the comparison will be aplles-to-apples\n",
        "# feel free to play with any of these numbers later\n",
        "max_trials=24\n",
        "num_initial_points=5\n",
        "beta=5.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhZNIJZ4RS5Y"
      },
      "source": [
        "#### Instantiate a `BayesianOptimization()` object for your grid search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33joO_J97mNs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9241d1ca-eed3-433f-9188-e854e808afd1"
      },
      "source": [
        "bayesian_tuner = BayesianOptimization(\n",
        "                    build_model,\n",
        "                    objective='val_accuracy',\n",
        "                    max_trials=max_trials,\n",
        "                    hyperparameters=hp, # pass in our hyperparameter dictionary\n",
        "                    num_initial_points=num_initial_points, \n",
        "                    beta=beta, \n",
        "                    seed=1234,\n",
        "                    directory='./keras-tuner-trial',\n",
        "                    project_name='bayesian_optimization_4')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Reloading Oracle from existing project ./keras-tuner-trial/bayesian_optimization_4/oracle.json\n",
            "INFO:tensorflow:Reloading Tuner from ./keras-tuner-trial/bayesian_optimization_4/tuner0.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9AM5Pdj7mNt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ca1e102-3836-4d46-c535-572f8a7f085c"
      },
      "source": [
        "bayesian_tuner.search(X_train, y_train,\n",
        "               epochs=3,\n",
        "               validation_data=(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 9 Complete [00h 00m 21s]\n",
            "val_accuracy: 0.8266800045967102\n",
            "\n",
            "Best val_accuracy So Far: 0.875\n",
            "Total elapsed time: 00h 03m 05s\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "FJcHC8d87mNt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a610cf67-6251-4976-9409-30a57ed02be9"
      },
      "source": [
        "bayesian_tuner.results_summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results summary\n",
            "Results in ./keras-tuner-trial/bayesian_optimization_4\n",
            "Showing 10 best trials\n",
            "Objective(name='val_accuracy', direction='max')\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 512\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.875\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 352\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8678799867630005\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 480\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.860319972038269\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 256\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8562399744987488\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 352\n",
            "learning_rate: 0.01\n",
            "activation: sigmoid\n",
            "Score: 0.8393200039863586\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 512\n",
            "learning_rate: 0.01\n",
            "activation: sigmoid\n",
            "Score: 0.8347200155258179\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 32\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8316400051116943\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 32\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.83024001121521\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 32\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8275200128555298\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 32\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8268799781799316\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woo9D9AU7mNu"
      },
      "source": [
        " ### Results\n",
        " \n",
        "Identify and write the the best performing hyperparameter combination and model score. \n",
        "Note that because this is  Bayesian Optimization, multiple runs might have slighly different outcomes. \n",
        " \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "1badcdca408cdd49bc2e409dca3bac5a",
          "grade": true,
          "grade_id": "cell-ff95600bf745f40f",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "1EXa47mH7mNu"
      },
      "source": [
        "Score: 0.875\n",
        "Trial summary\n",
        "Hyperparameters:\n",
        "units: 352\n",
        "learning_rate: 0.001\n",
        "activation: relu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOZ5-tJDraFE"
      },
      "source": [
        "We should point out that Gridsearch split the training set internally and created a test set whereas keras-tuner allows us to pass in a test set. This means that the keras-tuner algorithms were using one test set and our skearn GridSearchCV was using a different test set - so this isn't a perfectly exact 1-to-1 comparision but it'll have to do. In order to compensate for this, we did score the best model on the same test set that keras-tuner used. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPYChhrC7mNx"
      },
      "source": [
        "_______\n",
        "\n",
        "# Conclusion\n",
        "\n",
        "The spirit of this experiment is to expose you to the idea of benchmarking and comparing the trade-offs of various gridsearch approaches. \n",
        "\n",
        "Even if we did find a way to pass in the original test set into GridSearchCV, we can see that both Random Search and Bayesian Optimization are arguably better alternatives to a brute force grid search when we consider the trade-offs of run time and locating the best performing model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sth1AfwX7mNy"
      },
      "source": [
        "----\n",
        "\n",
        "# Stretch Goals\n",
        "\n",
        "- Feel free to run whatever gridsearch experiments on whatever models you like!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2APQG9H7mNy"
      },
      "source": [
        "# this is your open playground - be free to explore as you wish "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}